---
title: "1. script_properties"
author: "Fleur Cornelissen"
date: "2/7/2022"
output: html_document
---

```{r setup, include=FALSE}

# download libraries needed
library(xgboost)
library(drat)
library(DiagrammeR)
library(Matrix)
library(caret)
library(proxy)
library(e1071)
library(yardstick)
library(ggplot2)
library(caTools)
# setwd("/Users/fleurcornelissen/Dropbox/Paper BBB Permeability/NEW/Data/BBB")
library(Rtsne)
library(ggplot2)
library(dplyr)
library(readr)
library(tidyr)
library(stringr)
library(ggpubr)
library(gplots)
library(tidyverse)
library(Rtsne)
library(SHAPforxgboost)
library(pROC)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
# Download data (this is a dataset file with influx, bbb, efflux, cns and pampa combined)
aMF_df_raw <- read.csv("~/Dropbox/Paper BBB Permeability/NEW/Data/BBB/20220103_table.csv")
aMF_df <- aMF_df_raw[,c(1:14)] # select columns you want to have for analysis
aMF_df$ID <- as.numeric(1:8658) # Add ID number to each compound (easy to trace later in analysis)
aMF_df <- aMF_df[,c(15,1:14)]
rownames(aMF_df) <- aMF_df$ID # set ID number as rownames
aMF_df$HBA <- as.numeric(aMF_df$HBA) # set certain properties as numeric (necessary for analysis)
aMF_df$HBD <- as.numeric(aMF_df$HBD) # set certain properties as numeric (necessary for analysis)
aMF_df$RotatableBonds <- as.numeric(aMF_df$RotatableBonds) # set certain properties as numeric (necessary for analysis)

# Subset data (e.g. BBB separately for XGBoost analysis)
aMF_XG_BBB <- subset(aMF_df, aMF_df$Status.BBB == 0 | aMF_df$Status.BBB == 1) #select compounds of aMF_df datafile with Status.BBB = 0 or 1 (these are from BBB dataset, Status.BBB == NA is no BBB data)
aMF_XG_BBB$Status.BBB <- as.factor(aMF_XG_BBB$Status.BBB) # set Status.BBB as factor (necessary for analysis)
summary(aMF_XG_BBB$Status.BBB)

rownames(aMF_XG_BBB) <- aMF_XG_BBB$ID # again, set ID number as rownames (easier to trace compounds)

# Visualize datapoints with tSNE (https://datavizpyr.com/how-to-make-tsne-plot-in-r/)
# Let us select numerical columns using is.numeric() function with select(), standardise the data using scale() function before applying Rstne() function to perform tSNE.
tSNE <- aMF_XG_BBB[,c(8:15)] # select properties that you want to be your tSNE based on (here column 8-15 of aMF_XG_BBB dataset. do NOT add Status column obviously)

set.seed(142) #random numbers
tSNE_fit <- tSNE %>%
  select(where(is.numeric)) %>%
  Rtsne(check_duplicates = FALSE)

#The tSNE result object contains two tSNE components that we are interested in. We can extract the components and save it in a dataframe.
test <- tSNE_fit$Y %>% # extract coordinates (x and y) per compound on tSNE 
  as.data.frame() %>%
  rename(tSNE1="V1",
         tSNE2="V2") 

aMF_XG_BBB$x.p <- test$tSNE1 #add column x to dataset (tsne coordinate)
aMF_XG_BBB$y.p <- test$tSNE2 #add column y to dataset (tsne coordinate)

# Visualize tSNE
aMF_XG_BBB %>%
  ggplot(aes(x = x.p, 
             y = y.p,
             color = Status.BBB))+ # in this example tSNE coloured by Status (BBB permeable yes/no)
  geom_point(size=0.8) +
  theme_classic() + theme(legend.position = "none") + ggtitle("BBB Status")

#ggsave("tSNE_plot_example1.png") 
```

```{r cars}
# CREATE XGBOOST MODEL (If you split your dataset in a train/test/validation set for the first time, run the code between the hash-symbols. If already done (for properties model) you can ignore the blue part)

# 2. XGBoost

#############

# XGBoost dataset splitting#

#df <- aMF_XG_BBB[,c(6,8:15)]

# set.seed(3456)
# trainIndex <- createDataPartition(df$Status.BBB, p = .65, 
#                                 list = FALSE, 
#                                 times = 1)
# 
# XG_BBBtrain_c <- df[ trainIndex,] # train set 
# trainID <- as.data.frame(rownames(XG_BBBtrain_c)) # to match with original dataframe
# colnames(trainID)[1] <- "ID"
# 
# testval <- df[-trainIndex,] # split into test and validation 50%
# 
# set.seed(3456) # split into test and validation 50%
# trainIndex <- createDataPartition(testval$Status.BBB, p = .50, 
#                                 list = FALSE, 
#                                 times = 1)
# 
# XG_BBBtest_c  <- testval[-trainIndex,] # test set (402)
# testID <- as.data.frame(rownames(XG_BBBtest_c)) # to match with original dataframe
# colnames(testID)[1] <- "ID"
# 
# XG_BBBval_c <- testval[trainIndex,] #validation set (403)
# valID <- as.data.frame(rownames(XG_BBBval_c)) 
# colnames(valID)[1] <- "ID"
# 
# XG_BBBtest_c$set <- "Test"
# XG_BBBtrain_c$set <- "Train"
# XG_BBBval_c$set <- "Val"

#############

trainID <- subset(aMF_XG_BBB, aMF_XG_BBB$set == "Train") # select train compounds 
testID <- subset(aMF_XG_BBB, aMF_XG_BBB$set == "Test") # select test compounds 
valID <- subset(aMF_XG_BBB, aMF_XG_BBB$set == "Val") # select validation compounds 
# test <- rbind(trainID, testID, valID) # combine datasets again incl set column
# aMF_XG_BBB$set <- test$set[match(aMF_XG_BBB$ID, test$ID)]  #match IDs test with IDs complete dataset to add "set" column to complete dataset. Easy to split dataframe for comparison analysis for example (or MACCS)

XG_BBBtrain_c <- subset(aMF_XG_BBB[,c(6,8:15)], aMF_XG_BBB$set == "Train") # subset based on train/test/val
XG_BBBtest_c <- subset(aMF_XG_BBB[,c(6,8:15)], aMF_XG_BBB$set == "Test")
XG_BBBval_c <- subset(aMF_XG_BBB[,c(6,8:15)], aMF_XG_BBB$set == "Val")

XG_BBBtest_c$set <- "Test" # add set group (you can also add extra set column in previous step to skip this one)
XG_BBBtrain_c$set <- "Train"
XG_BBBval_c$set <- "Val"

XG_BBBtrain <- as.data.frame(XG_BBBtrain_c[,c(1:9)]) #train 
XG_BBBtest <- as.data.frame(XG_BBBtest_c[,c(1:9)]) #test
XG_BBBval <- as.data.frame(XG_BBBval_c[,c(1:9)]) #validation


# XGBoost analysis
# create dgcMatrix for analysis
sparse_matrix <- sparse.model.matrix(Status.BBB ~.-1, data = XG_BBBtrain) # dgcMatrix Training set
sparse_matrix_test <- sparse.model.matrix(Status.BBB ~.-1, data = XG_BBBtest) # dgcMatrix Test set

# create output vectors for test and training set
output_vector = XG_BBBtrain[,"Status.BBB"] == "1" # Status of train set TRUE = 1
output_vector_test = XG_BBBtest[,"Status.BBB"] == "1" # Status of test set TRUE = 1
output_vector2 = XG_BBBtrain[,"Status.BBB"] # for confusion matrix
output_vector_test2 = XG_BBBtest[,"Status.BBB"] # for confusion matrix

#build the model on training set
bst <- xgboost(data = sparse_matrix, label = output_vector, max_depth = 7,eta = 1, nthread = 1, nrounds = 3,objective = "binary:logistic", eval_metric='logloss', eval.metric = "error", eval.metric = "auc") # You can change nrounds, nthread, max depth etc to improve the model 

dtrain_sm <- xgb.DMatrix(data = sparse_matrix, label = output_vector)
dtest_sm <- xgb.DMatrix(data = sparse_matrix_test, label = output_vector_test)

#predict test data
pred <- predict(bst, sparse_matrix_test) 
print(length(pred)) # size prediction vector
print(head(pred)) # data results not binary yet
prediction <- as.numeric(pred > 0.5) #transform results to binary classification 
print(head(prediction)) #dubblecheck

#measuring model performance
err <- mean(as.numeric(pred > 0.5) != output_vector_test) # error model
print(paste("test-error=", err))

# Measuring learning progress with xgb.train
# Check the learning process of a model to test it on both training en test set
watchlist <- list(train=dtrain_sm, test=dtest_sm)
bst <- xgb.train(data=dtrain_sm, max_depth = 7,eta = 1, nthread = 1, nrounds = 3, watchlist=watchlist, eval.metric = "logloss", eval.metric = "error", eval.metric = "auc", objective = "binary:logistic")

#measure feature importance
importance <- xgb.importance(feature_names = colnames(sparse_matrix), model = bst)
head(importance) 

importanceRaw <- xgb.importance(feature_names = colnames(sparse_matrix), model = bst, data = sparse_matrix, label = output_vector)

# Cleaning for better display (not necessary)
importanceClean <- importanceRaw[,`:=`(Cover=NULL, Frequency=NULL)]
head(importanceClean) #In the table above we have removed two not needed columns and select only the first lines.Feature are al the splits used


tiff(file="MF_BBB_prop_feature.tiff", width=5, height=5, units="in", res=300) #save feature importance plot
xgb.plot.importance(importance_matrix = importance, top_n = 8) #plot
dev.off()

# Do the results make sense?
# higher chi2 means better correlation
c2 <- chisq.test(XG_BBBtrain$LogD, output_vector, simulate.p.value=T)
print(c2) # significant?

# View trees model
xgb.dump(bst, trees = 0, with_stats = TRUE) # tree 1 is "0" in code
xgb.plot.tree(model = bst) 

tiff(file="MF_BBB_prop_tree.tiff", width=5, height=5, units="in", res=300) # save trees plot
xgb.plot.tree(model = bst, trees = 0, show_node_id = TRUE)
dev.off()

# obtain prediction results of trainingset
pred_tr <- predict(bst, dtrain_sm)
print(head(pred_tr)) # data results not binary yet
label_prob_tr <- as.data.frame(pred_tr)
colnames(label_prob_tr) <- "BBB.Probability.properties" # PROBABILITY properties (i.e. raw data of results)

prediction_tr<- as.numeric(pred_tr > 0.5)#transform to binary classification
print(head(prediction_tr))
label_pred_tr<- as.data.frame(prediction_tr)
colnames(label_pred_tr) <- "BBB.Prediction.properties"  # PREDICTION properties (i.e. binary classification of results)

# Confusion matrices
confusionMatrix(table(ifelse(pred <= 0.5, 0, 1), output_vector_test2),positive = "1") #confusion matrix test set
confusionMatrix(table(ifelse(pred_tr <= 0.5, 0, 1), output_vector2),positive = "1") # confusion matrix training set


### VALIDATION
# create dgcMatrix for analysis
sparse_matrix_df_test <- sparse.model.matrix(Status.BBB ~.-1, data = XG_BBBval)

# create output vectors for test and training set
output_vector_df_test = XG_BBBval[,"Status.BBB"] == "1" # Status of test set TRUE = 1
output_vector_df_test2 = XG_BBBval[,"Status.BBB"]  # Status of test set TRUE = 1

df1_test <- xgb.DMatrix(data = sparse_matrix_df_test, label = output_vector_df_test)

#predict test data
pred_df <- predict(bst, sparse_matrix_df_test) 
print(length(pred_df)) # size prediction vector
print(head(pred_df)) # data results not binary yet
prediction_df <- as.numeric(pred_df > 0.5)#transform to binary classification
print(head(prediction_df))

#Confusion matrix
confusionMatrix(table(ifelse(pred_df <= 0.5, 0, 1), output_vector_df_test2), positive = "1")

#measuring model performance
err <- mean(as.numeric(pred_df > 0.5) != output_vector_df_test)
print(paste("test-error=", err))

# Check the learning process of a model to test it on both training en test set
watchlist <- list(train=dtrain_sm, test=df1_test)
bst <- xgb.train(data=dtrain_sm, max_depth = 7,eta = 1, nthread = 1, nrounds = 3, watchlist=watchlist, eval.metric = "error", eval.metric = "logloss", eval.metric = "auc", objective = "binary:logistic")

# load results into orignal dataframes
  #VAL
label_prob_df  <- as.data.frame(pred_df)
colnames(label_prob_df) <- "BBB.Probability.properties"
label_pred_df <- as.data.frame(prediction_df)
colnames(label_pred_df) <- "BBB.Prediction.properties"  

  #TEST
label_prob_te <- as.data.frame(pred)
colnames(label_prob_te) <- "BBB.Probability.properties" #S single (only tested on dataset)
label_pred_te  <- as.data.frame(prediction) #binary
colnames(label_pred_te) <- "BBB.Prediction.properties"

XG_BBBtrain_c1 <- cbind(XG_BBBtrain_c, label_prob_tr, label_pred_tr, trainID) #combine results
XG_BBBtest_c1 <- cbind(XG_BBBtest_c, label_prob_te, label_pred_te, testID)
XG_BBBval_c1 <- cbind(XG_BBBval_c, label_prob_df, label_pred_df, valID)

x <- rbind(XG_BBBtrain_c1[,c(10:13)],XG_BBBtest_c1[,c(10:13)], XG_BBBval_c1[,c(10:13)]) # bind prediction outcomes train/test/val (the columns you want, e.g. ID column, prediction and probability.properties column)

# add to original dataframe
aMF_XG_BBB$BBB.Probability.properties <- x$BBB.Probability.properties[match(aMF_XG_BBB$ID, x$ID)] # Match IDs
aMF_XG_BBB$BBB.Prediction.properties <- x$BBB.Prediction.properties[match(aMF_XG_BBB$ID, x$ID)] # Match IDs
# aMF_XG_BBB$set <- x$set[match(aMF_XG_BBB$ID, x$ID)] #add train/test/val (already done)

# add confusion matrix results
aMF_XG_BBB$CM.prop <- "FN"
aMF_XG_BBB$CM.prop[aMF_XG_BBB$BBB.Prediction.properties == "1" & aMF_XG_BBB$Status.BBB == 0] <- "FP" 
aMF_XG_BBB$CM.prop[aMF_XG_BBB$BBB.Prediction.properties == "0" & aMF_XG_BBB$Status.BBB == 0] <- "TN" 
aMF_XG_BBB$CM.prop[aMF_XG_BBB$BBB.Prediction.properties == "1" & aMF_XG_BBB$Status.BBB == 1] <- "TP" 
aMF_XG_BBB$CM.prop <- as.factor(aMF_XG_BBB$CM.prop)
summary(aMF_XG_BBB$CM.prop) #FN 50 FP 121 TN 387 TP 1719

 

```

```{r cars}
# PLOT RESULTS
# 3. Confusion matrix + ROC
# Training
set.seed(123)
truth_predicted <- data.frame(
  obs = output_vector2,
  pred = prediction_tr)
truth_predicted$obs <- as.factor(truth_predicted$obs)
truth_predicted$pred <- as.factor(truth_predicted$pred)

cm <- conf_mat(truth_predicted, obs, pred)

autoplot(cm, type = "heatmap") +
  scale_fill_gradient(low = "lightgrey", high = "coral1")

# test
set.seed(123)
truth_predicted <- data.frame(
  obs = output_vector_test2,
  pred = prediction)
truth_predicted$obs <- as.factor(truth_predicted$obs)
truth_predicted$pred <- as.factor(truth_predicted$pred)

cm <- conf_mat(truth_predicted, obs, pred)

autoplot(cm, type = "heatmap") +
  scale_fill_gradient(low ="lightgrey", high = "coral1")

# all datasets combined
set.seed(123)
truth_predicted <- data.frame(
  obs = output_vector_df_test2,
  pred = prediction_df)
truth_predicted$obs <- as.factor(truth_predicted$obs)
truth_predicted$pred <- as.factor(truth_predicted$pred)

cm <- conf_mat(truth_predicted, obs, pred)

autoplot(cm, type = "heatmap") +
  scale_fill_gradient(low = "lightgrey", high = "coral1")

#AUC
tiff(file="MF_BBB_prop_AUC.tiff", width=5, height=5, units="in", res=300)

par(pty = "s") #to obtain a nice squared graph
roc(XG_BBBval_c1$Status.BBB, XG_BBBval_c1$BBB.Probability.properties, plot=TRUE, legacy.axes=TRUE, percent = TRUE, xlab = "False Positive Percentage", ylab = "True Positive Percentage", col = "blue", lwd = "3", print.auc=TRUE, print.auc.y=30)
plot.roc(XG_BBBtrain_c1$Status.BBB, XG_BBBtrain_c1$BBB.Probability.properties, percent = TRUE, col = "darkgreen", lwd = "3", print.auc=TRUE, add=TRUE, print.auc.y=50) # Train set
plot.roc(XG_BBBtest_c1$Status.BBB, XG_BBBtest_c1$BBB.Probability.properties, percent = TRUE, col = "darkorange", lwd = "3", print.auc=TRUE, add=TRUE, print.auc.y=40) # test set
legend("bottomright", legend = c("Training", "Test", "Validation"), col = c("darkgreen", "darkorange", "blue"), lwd = "3") #create legend 
par(pty = "m")

dev.off()

#GPLOTS
aMF_XG_BBB %>% #Status
  ggplot(aes(x = x.p, 
             y = y.p,
             color = Status.BBB))+
  geom_point(size=1.8, alpha=0.6) +
  scale_color_manual(breaks = c("0", "1"),
  values=c("red3", "olivedrab")) + #assign colours
  theme_classic() + theme(legend.position = "none") + ggtitle(" ")

ggsave("MF_BBB_prop_status.png", units="in", width=8, height=7, dpi=300) # Save figure

aMF_XG_BBB %>% #classified groups
  ggplot(aes(x = x.p, 
             y = y.p,
             color = set, shape = Status.BBB))+
  geom_point(size=1.8) +
  scale_color_manual(breaks = c("Val", "Test", "Train"),
  values=c("blue", "darkorange", "darkgreen")) + #assign colours
  theme_classic() + theme(legend.position = "right") + ggtitle(" ")

ggsave("MF_BBB_prop_set.png", units="in", width=8, height=7, dpi=300) # Save figure

# GPLOTS
aMF_XG_BBB %>% # CM whole dataset
  ggplot(aes(x = x.p, 
             y = y.p,
             color = CM.prop))+
  geom_point(size=1.5) +
  scale_color_manual(breaks = c("TP", "TN", "FP", "FN"),
  values=c("coral3", "coral1", "ivory4", "lightgrey")) + #assign colours
  theme_classic() + theme(legend.position = "right") + ggtitle(" ")

ggsave("MF_BBB_prop_cm.png", units="in", width=8, height=7, dpi=300) # Save figure
 
```

```{r cars}
# SHAP 
# To return the SHAP values and ranked features by mean|SHAP|
shap_values <- shap.values(xgb_model = bst, X_train = sparse_matrix)

# The ranked features by mean |SHAP|
# a <- shap_values$mean_shap_score
# 
# a <- shap_values$mean_shap_score
# a <- as.data.frame(a)
# a[,2] <- rownames(a)
# ggplot(a, aes(x=V2, y=a, fill=V2)) +
#   geom_bar(stat="identity") + 
#  scale_fill_brewer("blues") + coord_flip() + theme_classic()

# SHAP values are calculated for each cell in the training dataset. The SHAP values dataset (shap_values$shap_score) has the same dimension (10148,9) as the dataset of the independent variables (10148,9) fit into the xgboost model.
# The sum of each row’s SHAP values (plus the BIAS column, which is like an intercept) is the predicted model output. As in the following table of SHAP values, rowSum equals the output predict(xgb_mod). I.e., the explanation’s attribution values sum up to the model output (last column in the table below). 

#Shap plots
#the summary plot shows global feature importance. The sina plots show the distribution of feature contributions to the model output (in this example, the predictions of CWV measurement error) using SHAP values of each feature for every observation. Each dot is an observation (station-day).
# To prepare the long-format data:
shap_long <- shap.prep(xgb_model = bst, X_train = as.matrix(sparse_matrix))

# **SHAP summary plot**
tiff(file="MF_BBB_prop_shap.tiff", width=6, height=6, units="in", res=300)
shap.plot.summary(shap_long) 
dev.off()

# Dependence plot
# g1 <- shap.plot.dependence(data_long = shap_long, x = 'TPSA', y = 'TPSA', color_feature = 'LogD') + ggtitle("(A) SHAP values of TPSA vs. TPSA")
# g2 <- shap.plot.dependence(data_long = shap_long, x = 'TPSA', y = 'LogD', color_feature = 'HBD') +  ggtitle("(B) SHAP values of LogD vs. TPSA")
gridExtra::grid.arrange(g1, g2, ncol = 2)

# Here I choose to plot top 4 features using function shap.plot.dependence.
# Plot SHAP value against feature value, without color_feature but has marginal distribution:
tiff(file="MF_BBB_prop_shap2.tiff", width=10, height=6, units="in", res=300)
fig_list <- lapply(names(shap_values$mean_shap_score)[1:8], 
                   shap.plot.dependence, data_long = shap_long)
gridExtra::grid.arrange(grobs = fig_list, ncol = 4)
dev.off()

# Interaction effects
# 
# SHAP interaction values separate the impact of variable into main effects and interaction effects. They add up roughly to the dependence plot.
# Quote paper 2: “SHAP interaction values can be interpreted as the difference between the SHAP values for feature i when feature j is present and the SHAP values for feature i when feature j is absent.”
# The SHAP interaction values take time since it calculates all the combinations.
# prepare the data using either: 
# (this step is slow since it calculates all the combinations of features.)
shap_int <- shap.prep.interaction(xgb_mod = bst, X_train = sparse_matrix)

# **SHAP interaction effect plot **
# if `data_int` is supplied, the same function will plot the interaction effect:
tiff(file="MF_BBB_prop_shap3.tiff", width=8, height=6, units="in", res=300)
g3 <- shap.plot.dependence(data_long = shap_long,
                           data_int = shap_int,
                           x= "TPSA", y = "LogD", 
                           color_feature = "LogD")
g4 <- shap.plot.dependence(data_long = shap_long,
                           data_int = shap_int,
                           x= "TPSA", y = "HBD", 
                           color_feature = "HBD")
g5 <- shap.plot.dependence(data_long = shap_long,
                           data_int = shap_int,
                           x= "TPSA", y = "MW", 
                           color_feature = "MW")
g6 <- shap.plot.dependence(data_long = shap_long,
                           data_int = shap_int,
                           x= "TPSA", y = "LogP", 
                           color_feature = "LogP")
gridExtra::grid.arrange(g3, g4, g5, g6, ncol=2)
dev.off()

# SHAP force plot
# The SHAP force plot basically stacks these SHAP values for each observation, and show how the final output was obtained as a sum of each predictor’s attributions.

# choose to show top 4 features by setting `top_n = 4`, 
# set 6 clustering groups of observations.  
plot_data <- shap.prep.stack.data(shap_contrib = shap_values$shap_score, top_n = 4, n_groups = 1)
# you may choose to zoom in at a location, and set y-axis limit using `y_parent_limit`  
shap.plot.force_plot(plot_data,zoom_in_location = 100)
#plot the clusters
#plot the clusters
tiff(file="MF_BBB_prop_shap4.tiff", width=8, height=6, units="in", res=300)
shap.plot.force_plot_bygroup(plot_data)
dev.off()
  
```

```{r cars}


```

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
